
**Intel® RealSense™ SDK for Linux**

**Developer Guide**

[Legal information 4](#legal-information)

[Welcome to the SDK 6](#welcome-to-the-sdk)

[Architecture 6](#architecture)

[SDK Versioning Scheme 9](#sdk-versioning-scheme)

[Compiling the SDK 10](#compiling-the-sdk)

[Build arguments 10](#build-arguments)

[Qt Creator 10](#qt-creator)

[Eclipse 11](#eclipse)

[Enabling Logging in Your Application 11](#enabling-logging-in-your-application)

[Enabling Testing in Your Application 12](#enabling-testing-in-your-application)

[SDK spatial correlation and projection
13](#sdk-spatial-correlation-and-projection)

[Record and Playback 16](#record-and-playback)

[Record 16](#record)

[Playback 16](#playback)

[Images 17](#images)

[SDK Pipeline 18](#sdk-pipeline)

[Using the Pipeline 18](#using-the-pipeline)

[Using the Pipeline With a single computer vision module
19](#using-the-pipeline-with-a-single-computer-vision-module)

[Using the Pipeline with Multiple Computer Vision Modules
21](#using-the-pipeline-with-multiple-computer-vision-modules)

[SDK Tools 22](#sdk-tools)

[Capture Tool 22](#capture-tool)

[Projection Tool 22](#projection-tool)

[System Info Tool 22](#system-info-tool)

[SDK utilities 23](#sdk-utilities)

[Log 23](#log)

[Time Sync 23](#time-sync)

[SDK Data Path 23](#sdk-data-path)

[FPS counter 24](#fps-counter)

[Samples 25](#samples)

[Record and Playback Sample 25](#record-and-playback-sample)

[Video Module Samples 25](#video-module-samples)

[Projection Sample 26](#projection-sample)

[Fatal Error Recovery 26](#fatal-error-recovery)

[Advanced topics 27](#advanced-topics)

[Working with the SDK on Windows 27](#working-with-the-sdk-on-windows)

[Changing the SDK 29](#changing-the-sdk)

Legal information
-----------------

By using this document, in addition to any agreements you have with Intel, you
accept the terms set forth below. You may not use or facilitate the use of this
document in connection with any infringement or other legal analysis concerning
Intel products described herein. You agree to grant Intel a non-exclusive,
royalty-free license to any patent claim thereafter drafted which includes
subject matter disclosed herein.

Information in this document is provided in connection with Intel products. No
license, express or implied, by estoppel or otherwise, to any intellectual
property rights is granted by this document. Except as provided in Intel's Terms
and Conditions of Sale for such products, Intel assumes no liability whatsoever
and Intel disclaims any express or implied warranty, relating to sale and/or use
of Intel products including liability or warranties relating to fitness for a
particular purpose, merchantability, or infringement of any patent, copyright or
other intellectual property right. Unless otherwise agreed in writing by Intel,
the Intel products are not designed nor intended for any application in which
the failure of the Intel product could create a situation where personal injury
or death may occur. Intel may make changes to specifications and product
descriptions at any time, without notice. Designers must not rely on the absence
or characteristics of any features or instructions marked "Reserved" or
"Undefined." Intel reserves these for future definition and shall have no
responsibility whatsoever for conflicts or incompatibilities arising from future
changes to them. The information here is subject to change without notice. Do
not finalize a design with this information.

The products described in this document may contain design defects or errors
known as errata which may cause the product to deviate from published
specifications. Current characterized errata are available on request.

Contact your local Intel sales office or your distributor to obtain the latest
specifications and before placing your product order. Copies of documents which
have an order number and are referenced in this document, or other Intel
literature, may be obtained by calling 1-800-548-4725, or by visiting Intel's
website <http://www.intel.com>.

Any software source code reprinted in this document is furnished under a
software license and may only be used or copied in accordance with the terms of
that license. Intel, the Intel logo, Intel RealSense, Intel Core and Iris are
trademarks or registered trademarks of Intel Corporation or its subsidiaries in
the United States and other countries.

Microsoft, Windows, and the Windows logo are trademarks, or registered
trademarks of Microsoft Corporation in the United States and/or other countries.
Java is a registered trademark of Oracle and/or its affiliates. OpenCL and the
OpenCL logo are trademarks of Apple Inc. used by permission by Khronos.

\*Other names and brands may be claimed as the property of others. Copyright ©
2010-2016, Intel Corporation. All rights reserved.

Welcome to the SDK
------------------

The Linux RealSense™ SDK provides libraries, tools and samples to develop
applications using Intel® RealSense™ cameras by using the librealsense API. The
SDK provides functionality of record and playback of camera streams for test and
validation. The SDK includes libraries which support the camera stream
projection of streams into a common world-space viewpoint, and libraries which
enable use of multiple middleware modules simultaneously for common multi-modal
scenarios.

Architecture
------------

![](media/fc6411f5317f88854f52afb365c12673.emf)

As shown in Figure 1, the SDK consists of several layers:

-   SDK Sample Applications

    -   *Samples* - The SDK provides sample code to demonstrate the usage of the
        SDK APIs. The samples consist of simple code snippets, without any
        external features such as visualization, GUI, command-line parameters,
        and rendering. The snippets represent most of the SDK API that is
        incorporated into customer applications.

    -   *Tools* - The SDK provides tools as source code to demonstrate usages of
        the various SDK libraries. The tools may be used for application or SDK
        testing and validation.

-   SDK Core

    -   *Image data container -* The SDK provides an image container for raw
        image access and basic image processing services, such as format
        conversion, mirror, rotation, and more. It caches the processing output
        to optimize multiple requests of the same operation. The image container
        includes image metadata, which may be used by any pipeline component to
        attach additional data or computer vision (CV) module processing output
        to be used by other pipeline components. The SDK uses a correlated
        samples container to provide access to camera images and motion sensor
        samples from the relevant streams, which are time synchronized. The
        correlated samples container includes all relevant raw buffers,
        metadata, and information required to access the attached images.

    -   *Spatial correlation and projection* - The SDK provides a library which
        executes spatial correlation and projection functions.

        The library provides the following facilities:

        -   Mapping of depth image to color image pixels, and mapping of color
            to depth.

        -   Projection and unprojection from depth or color image pixels to and
            from world coordinates.

        -   Creation of full images:

            -   UV map (depth to color)

            -   Inverse UV map (color to depth)

            -   Point cloud (same resolution as the depth stream)

            -   Color mapped to depth and depth mapped to color (same resolution
                as the corresponding stream)

    -   *Pipeline framework -* The SDK provides a pipeline framework, to expose
        a simpler developer API with focus on CV outputs, and hide the details
        of creating and running the application pipeline of the cameras and CV
        modules. The framework loads CV modules according to API configuration,
        aggregates modality requirements from the camera, and configures the
        camera with the selected profile to satisfy all loaded modalities. The
        framework executes the main loop of streaming from the camera and
        triggering the modalities.

-   Utilities - This library includes utilities, which can be used by SDK
    components and SDK applications.

-   Camera modules - The SDK provides recording and playback modules for
    application developer testing and validation. The modules are standalone
    utilities, which expose the camera API as defined by librealsense, and can
    replace the calls to the camera from the application.

SDK Versioning Scheme
---------------------

Versions are denoted using a standard triplet of integers: MAJOR.MINOR.PATCH

MAJOR versions are incompatible, large-scale upgrades of the API. Compatibility
with different major versions is not guaranteed.

MINOR versions retain source and binary compatibility with older minor versions.
Compatibility with prior minor versions is not guaranteed.

PATCH level are perfectly compatible, forwards and backwards.

-   "Source compatible" means that an application will continue to build without
    error, and that the semantics will remain unchanged.

-   "Binary compatible" means that a compiled application can be linked
    (possibly dynamically) against the library and will continue to function
    properly.

Compiling the SDK
-----------------

To successfully compile and use the SDK, you must install the following
dependencies:

-   librealsense

-   OpenCV 3.1

-   CMake

-   OpenGL GLFW version 3

-   Lz4 dev

-   Apache log4cxx – optional. Needed only if you want to enable logs.

Clone the SDK: git clone <https://github.com/IntelRealSense/realsense_sdk>.

### Build arguments

In a development environment, there are several build arguments that can be
helpful. The build arguments should be passed as arguments for CMake.

BUILD\_LOGGER //Set to ON to build logger.

BUILD\_TESTS //Set to ON if build tests should run, set to OFF to skip tests.

BUILD\_STATIC //Set to ON to build static libraries. Some libraries are always
shared.

BUILD\_PKG\_CONFIG //Set to ON if pkg-config tool should be built. More
information on pkg config can be found here.

CMAKE\_BUILD\_TYPE STREQUAL //Set to "Debug" to compile in Debug

### Qt Creator

1.  Download and install Qt Creator from theUbuntu software center.

2.  Open QT creator.

    1.  Choose Open Project, and select the file realsense\_sdk/CmakeLists.txt

3.  In the CMake wizard:

    1.  Choose build directory.

    2.  Choose build arguments:

        1.  BUILD\_LOGGER // Set to ON to build logger

        2.  BUILD\_TESTS // Set to ON if build tests should run, set to OFF to
            skip tests

        3.  BUILD\_STATIC // Set to ON to build static libraries. Some libraries
            are always shared

4.  In the **Edit** tab, you will see the realsense\_sdk folder and you can
    build the solution from there.

### Eclipse

1.  Install Eclipse by running the following command: sudo apt-get install
    eclipse-cdt

2.  Open Eclipse.

3.  Choose a workspace to work from.

4.  From the terminal:

    1.  Create a folder for the project and a cmake folder to run from:

        -   cd \~/workspaces

        -   mkdir someproject

        -   mkdir cmake

    2.  From the \~/workspaces/someproject/cmake folder, run the command:

        -   cmake -G "Eclipse CDT4 - Unix Makefiles" \~/realsense\_sdk

            Note: \~/realsense\_sdk is the location of the realsense\_sdk folder
            from the GitHub.

5.  In Eclipse, select the command:

    **File \> Import... \> General \> Existing Projects into Workspace**

    Check **Select root directory** and choose \~/workspaces/someproject/cmake.
    Make sure **Copy projects into workspace** is NOT checked.

    Click Finish.

6.  In the Project explorer windows, you will see the realse\_sdk folder and you
    can build the solution from there.

### Enabling Logging in Your Application

To enable logging in your application, perform the following steps:

1.  Install log4cxx by issuing the following command:

>   sudo apt-get install liblog4cxx10 liblog4cxx10-dev

1.  log does not compile by default. To compile it, add the -DBUILD\_LOGGER=ON
    option to CMake and then run make && make install.

2.  Copy the rslog.properties file to your \~/realsense/logs/ folder. You may
    copy it to any other directory you want. In that case, create an
    environmental variable REALSENSE\_SDK\_LOG\_PATH that points to that folder.

3.  Edit the rslog.properties file to match the output logs files you want to
    create. Root logger is the logger that always exists, but you may add your
    own logger. Pay attention to the log level hierarchy.

4.  Include the log\_utils.h header to your source files.

5.  Add realsense\_log\_utils to your link libraries
    (librealsense\_log\_utils.so).

6.  If you would like to write to the log from your application, add function
    calls by using defines from log\_utils.h to log. File name and line number
    will be logged automatically. For example:

>   LOG\_DEBUG ("This is my demo DEBUG message number %d with string %s\\n", 1,
>   "HELLO WORLD");

NOTE: Remove librealsense\_logger.so from /usr/local/lib for disabling logger.

NOTE: Due to ABI issues, log4cxx.so should be compiled with the same compiler
you use to compile the RealSense SDK. The default log4cxx package contains the
GCC 4.9 compiled library for Ubuntu 14.04 and the GCC 5.0 compiled library for
Ubuntu 16.04. Using different compilers might cause problems loading a shared
library. If you use a compiler version different from the default, compile
log4cxx from the source code and install it.

### Enabling Testing in Your Application

Tests do not compile by default. To compile them, add the - DBUILD\_TESTS =ON
option to CMake. Setting this flag to ON will compile Google gtest and compile
the SDK tests. After compilation, the tests will be available as part of the
solution. See the relevant IDE chapter for more details.

SDK spatial correlation and projection
--------------------------------------

The Spatial Correlation and Projection library provides utilities for spatial
mapping:

-   Mapping between color or depth image pixel coordinates and real world
    coordinates

-   Correlation of depth and color images and alignment in space

#### Pixel Coordinates

Each stream of images is associated with a separate 2D coordinate space,
specified in pixels, with the coordinate [0, 0] referring to the center of the
top left pixel in the image, and [w-1, h-1] referring to the center of the
bottom right pixel in an image containing exactly w columns and h rows. That is,
from the perspective of the camera, the x-axis points to the right and the
y-axis points down. Coordinates within this space are referred to as "pixel
coordinates", and are used to index into images to find the content of
particular pixels.

#### Real World Coordinates

Each stream of images is also associated with a separate 3D coordinate space,
specified in meters, with the coordinate [0, 0, 0] referring to the center of
the physical imager. The positive x-axis points to the right, the positive
y-axis points down, and the positive z-axis points forward. Coordinates within
this space are used to describe locations within the 3D space that might be
visible within a particular image.

The color images may originate in RealSense camera color sensor, external color
camera, or fisheye camera. The library requires the relevant sensors intrinsic
and extrinsic calibration parameters, along with the camera model.

![](media/b19941b9125583cf65da2ffa71070bd4.png)

#### Intrinsic Camera Parameters

The relationship between a stream's 2D and 3D coordinate systems is described by
its intrinsic camera parameters, contained in the intrinsics struct. The
intrinsic parameters describes how the images are produced by the camera, based
on the camera model, and the following assumptions:

-   **Images may be of arbitrary size**: The width and height fields describe
    the number of rows and columns in the image, respectively.

-   **The field of view of an image may vary**: The fx and fy fields describe
    the focal length of the image, as a multiple of pixel width and height.

-   **The pixels of an image are not necessarily square**: The fx and fy fields
    may be different (though they are commonly close).

-   **The center of projection is not necessarily the center of the image**:
    The ppx and ppy fields describe the pixel coordinates of the principal point
    (center of projection).

-   **The image may contain distortion**: The camera model describes which of
    several supported distortion models was used to calibrate the image, and the
    coefficients describe the distortion model.

The intrinsic camera parameters are required to carry out two fundamental
mapping operations:

-   **Projection**: Takes a point from a stream's 3D coordinate space, and maps
    it to a 2D pixel location on that stream's images.

-   **Unprojection**: Takes a 2D pixel location on a stream's images, as well as
    its depth, specified in meters, and maps it to a 3D point location within
    the stream's associated 3D coordinate space.

#### Distortion Models

Based on the camera device design, the different streams may be exposed via
different distortion models:

-   **None**: An image has no distortion, as though produced by an idealized
    pinhole camera. This is typically the result of some hardware or software
    algorithm undistorting an image produced by a physical imager, but may
    simply indicate that the image was derived from some other image or images
    which were already undistorted. Images with no distortion have closed-form
    formulas for both projection and unprojection.

-   **Modified Brown-Conrady Distortion**: An image is distorted, and has been
    calibrated according to a variation of the Brown-Conrady Distortion model.
    This model provides a closed-form formula to map from undistorted points to
    distorted points, while mapping in the other direction requires iteration or
    lookup tables.

-   **Inverse Brown-Conrady Distortion**: An image is distorted, and has been
    calibrated according to the inverse of the Brown-Conrady Distortion model.
    This model provides a closed-form formula to map from distorted points to
    undistorted points, while mapping in the other direction requires iteration
    or lookup tables.

#### Extrinsic Camera Parameters

The 3D coordinate system of each stream may in general be distinct. For
instance, it is common for depth to be generated from one or more infrared
imagers, while the color stream is provided by a separate color imager. The
affine transformations between the different streams coordinate systems assume
the following:

-   Imagers may be in separate locations, but are rigidly mounted on the same
    physical device. Therefore, transformation includes a 3D translation between
    the imager's physical positions, specified in meters.

-   Imagers may be oriented differently, but are rigidly mounted on the same
    physical device. Therefore, transformation includes an orthonormal 3D
    rotation, described by a 3x3 orthonormal rotation matrix between the
    imager's physical orientations.

-   All 3D coordinate systems are specified in meters. Therefore, there is no
    scaling in the transformation between two coordinate systems.

-   All coordinate systems are right-handed and have an orthogonal basis.
    Therefore, there is no mirroring or skewing in the transformation between
    two coordinate systems.

The extrinsic parameters between two streams allow mapping points from one
coordinate space to another. They are independent of the streams’ images.

The following operations are available using the library:

-   Map single pixels from depth image to color image.

-   Map single pixels from color image to depth image.

-   Project single pixels from color or depth image to real world coordinates.

-   Unproject single real world coordinates to color or depth image pixels.

-   Provide UV map image, where each pixel corresponds to a depth image pixel,
    and contains the normalized coordinates of the corresponding color image
    pixel. The image resolution equals the depth image resolution.

-   Provide inverse UV map image, where each pixel corresponds to a color image
    pixel, and contains the normalized coordinates of the corresponding depth
    image pixel. The image resolution equals the color image resolution.

-   Provide a point cloud image, with real world vertices. The image resolution
    equals the depth image resolution.

-   Provide depth image mapped to color image, where every depth pixel is mapped
    to the color image resolution. The output is a depth image, aligned in space
    and resolution to the color image.

-   Provide color image mapped to depth image, where every color pixel is
    retrieved from the color image, based on the UV map. The output is a color
    image, aligned in space and resolution to the depth image.

Record and Playback
-------------------

### Record

The record module provides a utility to create a file, which can be used by the
playback module to create a video source. The record module provides the same
camera API as defined by the SDK (librealsense) and the record API to configure
recording parameters such as output file and state (pause and resume).

The record module saves the following data to the output file:

-   Software components and file format versions.

-   Camera static information such as model, firmware version, serial number,
    and location.

-   Camera configuration information per stream: format, resolution, and FPS.

-   Camera option values: upon streaming start and upon changes during
    streaming, including calibration parameters for projection.

-   Camera images: raw buffer, timestamp, and metadata, upon read request from
    the application.

-   Motion sensor configuration.

-   Motion sensor samples.

The record module loads librealsense to access the camera device and execute the
set requests and reads, while writing the configuration and changes to the file.

The file writing may incur latency on the streams read loop. To prevent frame
drops, the disk access can be executed on a separate thread.

### Playback

The playback module provides a utility to create a video source from a file. The
playback module provides the same camera API as defined by the SDK
(librealsense), and the playback API to configure recording parameters such as
input file, playback mode, seek, and playback state (pause and resume).

The playback module supports the Windows RSSDK (RealSense SDK) file format as
well as the Linux SDK file format, to allow a wider variety of recorded files to
be used for validation.

Streaming from the file can be done in one of the following playback modes:

-   Real-time: provides images in the same intervals as captured from the
    camera.

-   As fast as possible: ignores the actual timestamp of the sample, but
    provides images in order.

Images
------

The correlated samples set represents the output of the camera. It includes
multiple images of the active streams, as well as samples of the active motion
sensors, which are time-synchronized. There are two types of synchronization:

-   Strong sync: Here, the stream images are produces by a single hardware
    trigger, providing minimal timestamp differences.

-   Software sync: Here, synchronization occurs between two independent imagers,
    based on the timestamp – in case of the same clock, used for both, or based
    on best effort, if no single clock reference is available.

It may include a single image or motion sample.

The image sample provides access to a single image with the following data:

-   Image information: Format, resolution, and pitches.

-   Timestamp: The image capture time, provided in the camera clock domain.

-   Image metadata: As provided by the camera stack or attached by the SDK CV
    modules

-   Raw buffer

-   Additional buffers: Cache of image processing outputs.

Image access is reference-counted, thus providing a lock and release API.

The image also provides common processing services, which may optimize execution
if called by the pipeline or CV modules more than once, as the processing output
is cached to buffers attached to the image. The following services are
available:

-   Format conversion, conversion to OpenCV matrix.

-   Depth precision conversion.

-   Image resize.

-   Rotate or mirror image.

-   UV map creation (while projection functions execute).

The motion sample provides a single motion sensor data and timestamp.

The CV module receives a correlated samples set when triggered to process
inputs, which includes the streams it requested upon configuration. The samples
set includes all streams samples if the CV module requested time synchronized
inputs, or the available streams samples if requested otherwise (may be a single
sample).

SDK Pipeline
------------

The pipeline is a class, which abstracts the details of how the cognitive data
is produced by the computer vision modules. This capability allows the
application to focus on consuming the computer vision output, leaving the camera
configuration and streaming details for the pipeline to handle. The application
merely has to configure the requested perceptual output, and handle the
cognitive data it gets during streaming.

If the application logic requires it, you can modify the pipeline source code,
to add additional features or processing operations to the camera samples. It
also allows developers to start with the simple API and extend the pipeline
control later, as they becomes familiar with the possibilities.

The pipeline executes the following flows to provide the abstraction:

-   Load CV modules based on the application request.

-   Consolidate and intersect requirements of CV module inputs, including
    trigger mode, with all images and samples, or as soon as any image or sample
    is available.

-   Select and configure camera(s) based on the required inputs intersection.

-   Fetch images and motion samples from the camera-relevant streams per CV
    module, in CV module FPS.

-   Time-synchronize images and motion samples, if this is not done by the
    camera module or the CV module, and aggregate all required inputs for each
    CV module prior to triggering it.

-   Trigger CV modules to process the required inputs, according to scheduling
    scheme (serial or parallel).

### Using the Pipeline

An application that uses the pipeline to produce cognitive data from one or more
computer vision modules should create a pipeline instance and instances of the
relevant CV modules. The application is responsible to manage the lifetime of
those modules. Once attached to the pipeline, the application can control the
streaming using the pipeline interface, and consume the CV modules data.

The application is responsible for:

-   Creating and configuring the computer vision modules.

-   Attaching the CV modules to the pipeline.

-   Optionally providing the pipeline a callback handler to be called upon the
    production of cognitive data produced or a camera device data becoming
    available. Alternatively, the application can use its own trigger to query
    for new data.

-   Starting or stopping pipeline streaming.

-   Accessing the computer vision modules to query for the cognitive data,
    regardless of the trigger for the query.

-   Managing the memory and lifetime of all modules, that is, computer vision
    and pipeline.

The pipeline has the following assumptions regarding attached computer vision
modules:

-   Each CV module implements the video module interface – the pipeline controls
    the modules using this API.

-   The CV module lifetime extends the pipeline lifetime – the pipeline assumes
    the CV module is valid as long as it is attached to the pipeline.

### Using the Pipeline With a single computer vision module

You can use the following pipeline\_sync\_sample code as a reference:

\#include \<iostream\>

\#include \<thread\>

\#include \<memory\>

\#include \<librealsense/rs.hpp\>

\#include "rs\_sdk.h"

\#include "rs/cv\_modules/max\_depth\_value\_module/max\_depth\_value\_module.h"

using namespace std;

using namespace rs::core;

using namespace rs::utils;

using namespace rs::cv\_modules;

class pipeline\_handler : public pipeline\_async\_interface::callback\_handler

{

public:

pipeline\_handler(int32\_t max\_depth\_module\_unique\_id) :

m\_max\_depth\_module\_unique\_id(max\_depth\_module\_unique\_id) {}

void on\_new\_sample\_set(const correlated\_sample\_set &sample\_set) override

{

//get a unique managed ownership of the image by calling add\_ref and wrapping
it with a

//unique\_ptr with a custom deleter which calls release.

rs::utils::unique\_ptr\<image\_interface\> depth\_image =
sample\_set.get\_unique(stream\_type::depth);

if(!depth\_image)

{

cerr\<\<"ERROR : got empty depth image"\<\<endl;

return;

}

cout\<\<"got depth image, frame number : " \<\<
depth\_image-\>query\_frame\_number() \<\<endl;

//do something with the depth image...

}

void on\_cv\_module\_process\_complete(video\_module\_interface \* cv\_module)
override

{

if(m\_max\_depth\_module\_unique\_id == cv\_module-\>query\_module\_uid())

{

auto max\_depth\_module =
dynamic\_cast\<rs::cv\_modules::max\_depth\_value\_module\*\>(cv\_module);

auto max\_depth\_data = max\_depth\_module-\>get\_max\_depth\_value\_data();

cout\<\<"max depth value : "\<\< max\_depth\_data.max\_depth\_value \<\< ",
frame number :"\<\< max\_depth\_data.frame\_number \<\<endl;

//do something with the max depth value...

}

//check the module unique id for other CV modules...

}

void on\_error(status status) override

{

cerr\<\<"ERROR : got pipeline error status : "\<\< status \<\<endl;

}

virtual \~pipeline\_handler() {}

private:

int32\_t m\_max\_depth\_module\_unique\_id;

};

int main () try

{

//create the CV module, implementing both the video\_module\_interface and a
specific CV module interface.

//the module must outlive the pipeline

std::shared\_ptr\<max\_depth\_value\_module\> module =
std::make\_shared\<max\_depth\_value\_module\>();

//create an async pipeline

std::unique\_ptr\<pipeline\_async\_interface\> pipeline(new pipeline\_async());

if(pipeline-\>add\_cv\_module(module.get()) \< status\_no\_error)

{

throw std::runtime\_error("failed to add cv module to the pipeline");

}

std::unique\_ptr\<pipeline\_handler\> pipeline\_callbacks\_handler(new
pipeline\_handler(module-\>query\_module\_uid()));

if(pipeline-\>start(pipeline\_callbacks\_handler.get()) \< status\_no\_error)

{

throw std::runtime\_error("failed to start pipeline");

}

//sleep to let the CV module get some samples

std::this\_thread::sleep\_for(std::chrono::seconds(5));

if(pipeline-\>stop() \< status\_no\_error)

{

throw std::runtime\_error("failed to start pipeline");

}

return EXIT\_SUCCESS;

}

catch (std::exception &e)

{

cerr\<\<e.what()\<\<endl;

return EXIT\_FAILURE;

}

### Using the Pipeline with Multiple Computer Vision Modules

The creation of the pipeline for multiple CV modules is similar. Notice that all
CV modules share the same pipeline instance, as well as the same callback
handler, if provided. The application can query the callback CV module parameter
for its unique ID, to detect which CV module output is available.

SDK Tools
---------

The SDK provides tools, which can be used by application developers for testing
and validation of their application and execution environment.

### Capture Tool

The capture tool provides a GUI to view camera streams, create a new file from a
live camera, and play a file if it is in one of the supported formats. The tool
provides options to render the camera or file images.

### Projection Tool

The Projection tool provides simple visualization of the projection functions
output, to allow human eye detection of major offsets in the projection
computation.

### System Info Tool

The System Info tool presents the following system data:

-   Linux name

-   Linux kernel version

-   CPU information

-   Information about cameras:

    -   Number of cameras

    -   Camera name

    -   Camera firmware

-   Version of Intel RealSense SDK for Linux

-   g++ version

-   Information about librealsense.so:

    -   Whether librealsense.so exists in the right place

    -   The librealsense.so version

-   Data from PATH, LD\_LIBRARY\_PATH, and INCLUDE environment variables

-   Name and version of the OpenCV package

-   Name and version of the Apache log4cxx package

-   Name and version of the libjpeg package

SDK utilities
-------------

The SDK provides utilities, which can be used by the application developer to
simplify non-trivial tasks. The utilities are used by the SDK components
internally, but may be beneficial for the application to use.

### Log

The SDK provide a logging library, which can be used by the SDK components and
the application to log meaningful events.

The log library is optional for the application, and in the absence of the
library, a default empty logger is applied on the code-logging requests.

The utility provides:

-   Options for different log levels.

-   Print messages with parameters.

-   The ability to select different output targets.

The log utility is a wrapper over the Apache log4cxx library, which is
cross-platform. Documentation for log4cxx can be found
[here](https://logging.apache.org/log4cxx/latest_stable/).

### Time Sync

The Time Sync utility provides methods to synchronize multiple streams of images
and motion samples, based on the sample timestamp or sample number.

The utility:

-   Receives the samples from the different streams independently, possibly on
    different threads concurrently.

-   Matches samples with minimal time-gap to a correlated samples set.

-   Incurs the minimal required latency to match the samples.

In the case of the ZR300 camera, fisheye matching requires the camera strobe
option to be set, to provide well-synchronized streams. In the absence of the
strobe setting, there is no guarantee of the matching behavior.

### SDK Data Path

The SDK provides a utility to locate SDK files in the system.

The utility is used by CV modules, which need to locate data files in the system
that are constant for all applications (not application- or algorithm-instance
specific).

The data files are searched in the following order:

1. Local folder of the application: used for internal development.

2. Dedicated environment variable: allows you to select a location.

3. Predefined directory: /opt/intel/rssdk/data

The utility can be linked by the CV modules as a static library.

### FPS counter

-   Measure actual FPS in the same manner in all SW stack layers

-   Make FPS measurements available to automation

-   Make FPS measurements available to sample applications

-   The FPS measure solution shall support actual FPS report to the using module

    -   Expose "query actual FPS" interface

-   The FPS measure solution shall support actual FPS recording to a file, for
    automation or external GUI

-   When FPS measurement is disabled - no additional overhead shall be added

-   When FPS measurement is enabled - the overhead shall not modify the
    available FPS

Samples
-------

There are basic and simple code samples that demonstrate how to use the SDK API:

-   Streams spatial correlation sample

-   Playback sample

-   Recording sample

The samples conform to the following common guidelines:

-   The samples demonstrate clear usage of the SDK and dependent components API,
    in a way that you can reuse the code by using copy and paste.

-   The samples exhaust most of the SDK component API and functionality.

-   The samples do not mix SDK usage with application-specific features, such as
    GUI, command line parameters, interaction with the user, and so on. If
    additional functionality is required to demonstrate the usage, the code is
    separated to different functions which use the SDK API.

-   If there are multiple ways to execute the same functionality. The sample
    demonstrates all options (with reasonable limit, assuming the SDK design
    limits the number of ways to do the same thing).

-   The samples demonstrate simple and advanced uses of the SDK API.

-   The samples are well commented to explain the reasoning of an advanced
    usage.

-   The samples adhere to common coding guidelines.

-   The samples include a main function which returns 0 on success, or -1 on API
    failures. This is used to detect a broken sample.

### Record and Playback Sample

The sample demonstrates how to record and play back a file while the application
is streaming, with and without an active CV module, with minimal changes to the
application compared to live streaming.

### Video Module Samples

#### Video Module Asynchronized Sample

The sample demonstrates an application usage of a Computer Vision module, which
implements asynchronous sample processing. The video module implements the video
module interface, which is a common way for the application or SDK to interact
with the module. It also implements a module-specific interface. In this
example, the module calculates the maximal depth value in the image.

#### Video Module Synchronized Sample

The sample demonstrates an application usage of a Computer Vision module, which
implements synchronous samples processing.

The video module implements the video module interface, which is a common way
for the application or SDK to interact with the module. It also implements a
module-specific interface. In this example, the module calculates the maximal
depth value in the image.

### Projection Sample

The sample demonstrates how to use the different spatial correlation and
projection functions, from live camera and recorded file.

### Fatal Error Recovery

The sample demonstrates how the application can recover from fatal error in one
of the SDK components (CV module or core module), without having to terminate.


Advanced topics
---------------

### Working with the SDK on Windows

The SDK is cross-platform and can also compile and run on Windows.

This section provides instructions on working with the SDK in Microsoft Visual
Studio. To continue, make sure you have successfully installed Visual Studio
2015.

To successfully compile and use the SDK, install the following dependencies:

-   Git Bash: You can download it from: <https://git-scm.com/download/win>

-   Librealsense: Follow instructions from
    <https://github.com/IntelRealSense/librealsense/blob/master/doc/installation.md>

-   OpenCV3.1: You can download OpenCV from here (choose version 3.1):
    <http://opencv.org/downloads.html>

-   CMake: You can download it from here: <https://cmake.org/download/> Choose
    the binary distribution for the Windows 64Bit Platform
    ([cmake-3.6.1-win64-x64.msi](https://cmake.org/files/v3.6/cmake-3.6.1-win64-x64.msi)).

-   OpenGL GLFW: OpenGL comes with the system. However, you will need to check
    if you have the recent driver for your graphics hardware. Visit this link:
    <https://www.opengl.org/wiki/Getting_Started#Downloading_OpenGL> to get the
    appropriate Windows driver for your graphic card.

    Choose your graphics hardware manufacture under **Downloading OpenGL \>
    Windows**.

-   lz4

-   Apache log4cxxis optional. It is required only if you would like to enable
    logs.

-   Google Gtest is optional. It is required only if you would like to enable
    tests.

    To install Gtest, perform the following steps:

1.  git clone <https://github.com/google/googletest.git>

2.  mkdir cmakedir

3.  cd cmakedir

4.  cmake ../googletest

5.  Build the .sln file.

From Git Bash, run: git clone <https://github.com/IntelRealSense/realsense_sdk>.

After installing all necessary dependencies and cloning the SDK source code,
create a solution you can work with in Visual Studio. To create a solution file,
work with CMake in one of the following working options:

1.  **working with Cmake Gui:**

2.  [./media/image3.JPG](./media/image3.JPG)

    Run CMake:

3.  Click **Browse Source**, as shown in the image above.

4.  Choose the path to your source directory.

5.  Click **Browse Build**, as shown in the image above.

6.  Choose the path to your output directory (the .sln file will create in this
    folder).

7.  Click **Configure**.

8.  In the new window that appears, select **Visual Studio 14 2015 Win64** in
    the **Specify the generator for this project** list.

    ![](media/baa985bab13c23a103813bd717e73354.jpg)

9.  Click **Generate**.

10. Go to the path that you selected in step 5 and open the new solution file.

11. **Working without a Gui:**

12. From the command line, run the following command:

    cmake \<[specify build arguments](#_Build_arguments)\> -G “Visual Studio 14
    2015 Win64” \<path-to-source-dir\> [-DLIBREALSENSE\_DIR=\<path\>]

    **Note**: Defining a directories path is optional. The default path is: C:/
    realsense/3rdparty.

13. Open your destination directory as defined above and run the .sln file.

>   For more options on working with CMake without a GUI, see the relevant
>   documentation under:

>   <https://cmake.org/cmake/help/v3.6/manual/cmake.1.html>

Note: The generated .sln file can be opened in Visual Studio, just as any other
.sln file.

#### Using playback on Windows

Any file format that was recorded using the SDK on Linux can also be played in
Windows. Follow the previous instructions and compile the capture tool. Using
the compiled capture tool, you can play every SDK pre-recorded clip.

To view online help on the tool, run the tool with the –h argument.

### Changing the SDK

You can change the SDK by debugging the code. For debug purpose you will need to
set the cmake build argument CMAKE\_BUILD\_TYPE STREQUAL to Debug.

If you think that the changes, elaborations, or upgrades you made to the SDK are
relevant for other users, you may submit a pull request at
<https://github.com/IntelRealSense/realsense_sdk/pulls>.

The SDK maintainers will review your contribution and, if any additional fixes
or modifications are necessary, might give some feedback to guide you. When
accepted, your pull request will be merged into the Intel RealSense GitHub
repository.

Intel RealSense SDK is licensed under [Apache License, Version
2.0](http://www.apache.org/licenses/LICENSE-2.0). By contributing to the
project, you agree to the license and copyright terms therein and release your
contribution under these terms.

#### Validating your changes

The best way to validate SDK changes is to run the SDK tests and make sure that
they pass. Any changes to the SDK might drag necessary adjustments to some
tests.

Tests can be found under
<https://github.com/IntelRealSense/realsense_sdk/tree/master/tests>.

To compile and work with the tests in you development environment, please read
[here](#_How_to_enable).

#### Using multiple SDK versions 

Multiple major SDK versions can be installed on the same machine simultaneously.

To choose a specific version to link to, you must specify the version number of
the .so file. If you are linking to a .so file without a version number, the .so
will be the same version of the development package installed on the machine.

